{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0d2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "from pytube import YouTube \n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd2979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/yt_cats_cnn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e448892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ../tmp_sliced_audio/audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def extract_audio_clip(video_path, file_name, start_time=None, end_time=None):\n",
    "    ''' Take video path, start and end times as input and output .wav audio clip to the specified path'''\n",
    "       \n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "\n",
    "    # Set the start and end times\n",
    "    start_time = start_time or 0\n",
    "    end_time = end_time or video_clip.duration\n",
    "\n",
    "    # Get the audio from the video for the specified time interval\n",
    "    audio_clip = video_clip.subclip(start_time, end_time).audio\n",
    "\n",
    "    # Write the audio to a WAV file\n",
    "    audio_clip.write_audiofile('../tmp_sliced_audio/'+ file_name)\n",
    "\n",
    "    # Close the video and audio clips\n",
    "    video_clip.close()\n",
    "    audio_clip.close()\n",
    "\n",
    "extract_audio_clip('../uploaded_video/court_tmo_interview.mp4', 'audio.wav', 20, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c992ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_audio_clips(clips):\n",
    "    X = []\n",
    "    for clip in clips:\n",
    "        # Convert audio to spectrogram\n",
    "        spectrogram = # Your code to generate spectrogram\n",
    "        \n",
    "        # Preprocess spectrogram\n",
    "        preprocessed_spectrogram = # Your code to preprocess spectrogram\n",
    "        \n",
    "        X.append(preprocessed_spectrogram)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Run predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Get predicted classes\n",
    "    predicted_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    return predicted_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853fccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_likely_clips(clips, predicted_classes, video_path):\n",
    "    clips_by_category = {}\n",
    "    for i, clip in enumerate(clips):\n",
    "        category = predicted_classes[i]\n",
    "        if category not in clips_by_category:\n",
    "            clips_by_category[category] = []\n",
    "        clips_by_category[category].append((clip, i))\n",
    "    \n",
    "    video = VideoFileClip(video_path)\n",
    "    \n",
    "    clips_with_video_timing = {}\n",
    "    \n",
    "    for category, clips in clips_by_category.items():\n",
    "        clips_with_video_timing[category] = []\n",
    "        for clip, clip_idx in clips:\n",
    "            start_time = clip_idx * (clip.duration - overlap) / 1000\n",
    "            end_time = (clip_idx + 1) * (clip.duration - overlap) / 1000\n",
    "            video_clip = video.subclip(start_time, end_time)\n",
    "            clips_with_video_timing[category].append(video_clip)\n",
    "    \n",
    "    return clips_with_video_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60daf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_video(video_path, clip_length, overlap):\n",
    "    audio = extract_audio(video_path, 0, None)\n",
    "    clips = generate_audio_clips(audio, clip_length, overlap)\n",
    "    predicted_classes = classify_audio_clips(clips)\n",
    "    most_likely_clips = find_most_likely_clips(clips, predicted_classes)\n",
    "    return most_likely_clips"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

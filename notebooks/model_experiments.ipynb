{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a9541-d4a9-467b-a627-0186fe7c9792",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37ad94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from itertools import islice\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411d853",
   "metadata": {},
   "source": [
    "### GC storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb40af",
   "metadata": {},
   "source": [
    "Initialise connection to google cloud bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00ebbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket('soundboard_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "484e9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_path = \"gs://your-bucket-name/your-audio-file.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2614ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = storage_client.list_blobs('soundboard_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf38b3e",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8928d34",
   "metadata": {
    "tags": []
   },
   "source": [
    "Binary classification model to predict whether each time-frequency bin in the spectrogram belongs to the percussive component or not. Then we can use the predicted mask to extract the percussive elements from the original audio. Map keys to tags by converting csv to dataframe to dict. \n",
    "\n",
    "BIG thanks to ChatGPT\n",
    "\n",
    "The first sample in the output is the original, the second sample is the NN's attempt at predicting where the percussion happens.\n",
    "\n",
    "Almost no effect for any of them except for the second third, where you can hopefully hear (probably only with headphones) that the noise around the percussive elements is reduced. This is definitely something to play with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "726e9253-9bf0-404e-88a4-430b6bfcb455",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Iterator has already started', <google.api_core.page_iterator.HTTPIterator object at 0x7fb2d79f50d0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_15909/1726528940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display the i audio files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'File {i}: {blob.name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/jupyterlab/lib/python3.7/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iterator has already started\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Iterator has already started', <google.api_core.page_iterator.HTTPIterator object at 0x7fb2d79f50d0>)"
     ]
    }
   ],
   "source": [
    "# Display the i audio files\n",
    "for i, blob in enumerate(blobs):\n",
    "        if i>4:\n",
    "            break\n",
    "        print(f'File {i}: {blob.name}')\n",
    "        # Download the audio file to a local file\n",
    "        file_path = os.path.join('../data/temp_audio', blob.name.split('/')[-1])\n",
    "        blob.download_to_filename(file_path)\n",
    "\n",
    "        # Load the audio file with librosa\n",
    "        y, sr = librosa.load(file_path, sr=44100)\n",
    "\n",
    "        # Display and play the audio file\n",
    "        #display(Audio(y, rate=sr))\n",
    "        from IPython.display import Audio\n",
    "\n",
    "        # Compute spectrogram\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        spec = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "        mag = np.abs(spec)\n",
    "\n",
    "        # Define target labels\n",
    "        target = np.zeros_like(mag)\n",
    "        target[:512,:] = 1.0\n",
    "\n",
    "        # Normalize magnitude component\n",
    "        mag_db = librosa.power_to_db(mag)\n",
    "        mag_norm = (mag_db - np.min(mag_db)) / (np.max(mag_db) - np.min(mag_db))\n",
    "\n",
    "        # Convert to TensorFlow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((mag_norm.T, target.T))\n",
    "\n",
    "        # Define model architecture\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(1025, input_shape=(1025,), activation='relu'),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(1025, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(dataset.batch(32), epochs=50)\n",
    "\n",
    "        # Use model to separate percussive elements\n",
    "        pred = model.predict(mag_norm.T)\n",
    "        percussive_mag = pred.T * mag\n",
    "\n",
    "        # Inverse STFT to obtain percussive signal\n",
    "        percussive_spec = percussive_mag * np.exp(1j * np.angle(spec))\n",
    "        percussive_signal = librosa.istft(percussive_spec, hop_length=hop_length)\n",
    "\n",
    "        # Display original and percussive audio\n",
    "        display(Audio(y, rate=sr))\n",
    "        display(Audio(percussive_signal, rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcf150d-449c-448b-990e-9463065165dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d3957-7909-4e82-8a30-ef93df589373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m105",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m105"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
